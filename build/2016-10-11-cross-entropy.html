<!DOCTYPE html>
<html class="no-js">
  <head>
	<meta charset="utf-8">
	<title>Derivative free reinforcement learning with the cross entropy method | Dustin Tran</title>
	<meta name="description" content="">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-Frame-Options" content="sameorigin">

	<!-- CSS -->
	<link rel="stylesheet" href="/blog/css/main.css">

	<!--Favicon-->
	<link rel="shortcut icon" href="/blog/favicon.ico" type="image/x-icon">

	<!-- Canonical -->
	<link rel="canonical" href="http://dustintran.com/blog/2016-10-11-cross-entropy">

	<!-- RSS -->
	<link rel="alternate" type="application/atom+xml" title="Dustin Tran" href="http://dustintran.com/blog/feed.xml" />

	<!-- Font Awesome -->
	<link href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet">

	<!-- Google Fonts -->
	
	<link href="//fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,700italic,400italic" rel="stylesheet" type="text/css">
	

	<!-- KaTeX -->
	
	<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.css">
	<script src="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.js"></script>
	

	<!-- Google Analytics -->
	
	<script>
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

	ga('create', 'UA-53150914-2', 'auto');
	ga('send', 'pageview');
	</script>
	
</head>

  <body>
    <header class="site-header">
	<div class="branding">
		
		<a href="/blog/">
			<img class="avatar" src="/blog/img/photo.png" alt=""/>
		</a>
		
		<h1 class="site-title">
			<a href="/blog/">Dustin Tran</a>
		</h1>
	</div>
	<nav class="site-nav">
		<ul>
                        <li>
				<a class="page-link" href="http://dustintran.com">
					about　
				</a>
			</li>
			
			
			<li>
				<a class="page-link" href="/blog/2016-10-11-cross-entropy">
					Derivative free reinforcement learning with the cross entropy method
				</a>
			</li>
			
			
			
			
			
			
			
			
			
			
			<!-- Social icons from Font Awesome, if enabled -->
			














<li>
	<a href="https://github.com/dustinvtran" title="Follow on GitHub">
		<i class="fa fa-fw fa-github"></i>
	</a>
</li>





















<li>
	<a href="https://twitter.com/dustinvtran" title="Follow on Twitter">
		<i class="fa fa-fw fa-twitter"></i>
	</a>
</li>






		</ul>
	</nav>
</header>

    <div class="content">
      <article >
  <header style="background-image: url('/blog/')">
    <h1 class="title">Derivative free reinforcement learning with the cross entropy method</h1>
    <div style="padding-top:3%;"></div>
    <div class="meta">
      <span class="author">
        By
        
          <a class="author-name" href="http://dustintran.com">Dustin Tran</a>
        
      </span>
      <span class="date"></span>
      <ul>
        














<li>
	<a href="https://github.com/dustinvtran" title="Follow on GitHub">
		<i class="fa fa-fw fa-github"></i>
	</a>
</li>





















<li>
	<a href="https://twitter.com/dustinvtran" title="Follow on Twitter">
		<i class="fa fa-fw fa-twitter"></i>
	</a>
</li>






      </ul>
    </div>
  </header>
  <section class="post-content"><p>Modern reinforcement learning can be split up into 3 main approaches:</p>

<ol>
  <li><a href="http://www.control.ece.ntua.gr/UndergraduateCourses/ProxTexnSAE/Bertsekas.pdf">Policy iteration methods</a> which alternate between estimating the value function under the current policy and improving the policy.</li>
  <li><a href="http://is.tuebingen.mpg.de/fileadmin/user_upload/files/publications/Neural-Netw-2008-21-682_4867[0].pdf">Policy gradient methods</a> which use an estimator of the gradient of the expected return (total reward) obtained from sample trajectories.</li>
  <li><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.81.6579&amp;rep=rep1&amp;type=pdf">Derivative free approaches</a> which treat the return as a black box function to be optimized in terms of the policy parameters.</li>
</ol>

<p>In this post we’ll walk through an implementation of the cross-entropy optimization method on the <a href="https://gym.openai.com/envs/CartPole-v0">OpenAI Cartpole environment</a>.</p>

<h3 id="introduction-to-rl">Introduction to RL</h3>

<p>The reinforcement learning setting consists of an agent and an environment. At every timestep, the agent chooses an action, and the environment returns a reward and transitions into the next state. We assume the standard, fully-observed setting as described in the excellent OpenAI’s excellent <a href="https://gym.openai.com/docs/rl">Deep Reinforcement Learning Tutorial</a>.</p>

<h2 id="problem-setup">Problem Setup</h2>
<p>To briefly summarize, at time <script type="math/tex">t = 0</script> we have some initial state <script type="math/tex">s_0</script> and observation <script type="math/tex">y_0</script>. We pick an action $a_0$ according to a policy <script type="math/tex">\pi_\theta(a_0 \mid s_0)</script>. That is, <script type="math/tex">\pi_\theta(a_0 \mid s_0)</script> is a probability distribution conditioned on <script type="math/tex">s_t</script> and parameterized by <script type="math/tex">\theta</script>.</p>

<p>At time <script type="math/tex">t>0</script> until the end of the episode, we get a new state <script type="math/tex">s_t</script>, observation <script type="math/tex">y_t</script>, and reward from the previous time step <script type="math/tex">r_{t-1}</script>. After some number of episodes (eg. a rollout), we update our policy <script type="math/tex">\pi_\theta(a_t \mid s_t)</script> based on the cumulative reward generated from those episodes.</p>

<h3 id="environment">Environment</h3>

<p>In the cartpole environment, we have a 4 dimensional observation vector <script type="math/tex">o_t \in \mathbb{R^4}</script> that contains the following features:
- cart position
- pole_angle
- cart_velocity
- angle_rate_of_change</p>

<h2 id="algorithm">Algorithm</h2>
<p>Let <script type="math/tex">d</script> be the number of dimensions in the model, <script type="math/tex">N</script> be the batch size of each rollout, and <script type="math/tex">k</script> be the number of iterations of our algorithm.</p>

<blockquote>
  <p>Initialize <script type="math/tex">\mu \in \mathbb{R}^d, \sigma \in \mathbb{R}^d</script> <br />
For iteration <script type="math/tex">i \in \{0,\ldots,k\}</script>:</p>

  <blockquote>
    <p>Collect <script type="math/tex">N</script> samples of <script type="math/tex">\theta_i \sim \mathcal{N}(\mu_i, \text{diag}(\sigma_i))</script> <br />
Perform a noisy evaluation <script type="math/tex">f(\theta_i, \zeta_i)</script> on each one <br />
Select the top <script type="math/tex">p\%</script> of samples, which we’ll call the <strong>elite set</strong> <br />
Fit a Gaussian distribution, with diagonal covariance, to the <strong>elite set</strong>, obtaining a new <script type="math/tex">\mu</script>, <script type="math/tex">\sigma</script>. <br /></p>
  </blockquote>
</blockquote>

<h2 id="convergence">Convergence</h2>

<p>To analyze the convergence of CEM, we must first consider the Monte Carlo Expectation Maximization (MCEM) problem, which solves a Maximum Likelihood problem of the form:</p>

<script type="math/tex; mode=display">\max_\theta \log \int_z p(y,z \mid \theta)dz</script>

<p>where <script type="math/tex">\theta</script> is the parameters of the probabilistic model we are trying to find, <script type="math/tex">y</script> is the observed variables and <script type="math/tex">z</script> is the unobserved variables.</p>

<p>If <script type="math/tex">z</script> were known, we could use a simple Maximum A Posterior (MAP) estimation with a prior on <script type="math/tex">\theta</script> and be done. However, this is not the case, so we have to use a Monte Carlo estimate of <script type="math/tex">z</script> based on the posterior <script type="math/tex">p(z \mid y,\theta)</script>. The algorithm works by iteratively estimating <script type="math/tex">E(\log p(y,z \mid \theta_k) \mid Y)</script> using the sample values of <script type="math/tex">z</script>, maximizing it with respect to <script type="math/tex">\theta_k</script>, and then using this new <script type="math/tex">\theta_{k+1}</script> in the following timestep. At each timestep <script type="math/tex">k</script>, for some sample size <script type="math/tex">M</script> we have</p>

<script type="math/tex; mode=display">E(\log p(y,z \mid \theta_k)) = \dfrac{1}{M} \sum_{m=1}^M \log p(y, z_m \mid \theta_k)</script>

<p>In each iteration, we’re collecting <script type="math/tex">m</script> samples, using these to form a distribution over our latent variables <script type="math/tex">z</script>, and then reweighting this distribution by maximizing the likelihood of those samples. This is very similar to CEM except that in CEM and other RL problems we’re trying to maximize expected reward and we don’t know what samples (actions) lead to a good reward. More precisely, MCEM is maximizing the expected value of the likelihood function <script type="math/tex">l(\theta;X)</script>, and in CEM we’re maximizing the expected value of the reward function, which we write as:</p>

<script type="math/tex; mode=display">E(R) = R \log p(y,z \mid \theta_k)</script>

<p>where <script type="math/tex">R = \sum_{m=1}^M r_m</script> is the cumulative reward over <script type="math/tex">M</script> samples and <script type="math/tex">y$,</script>z$$ are both sequences of state-action pairs. Thus, our Monte-Carlo estimation function becomes:</p>

<script type="math/tex; mode=display">\dfrac{1}{M} \sum_{m=1}^M \log p(y,z_m \mid \theta_k) r_m</script>

<p>Theorem: CEM converges to a local maximum of the objective <script type="math/tex">N(\theta) = E(f(\theta))</script>
Theorem: CEM does not converge to a local maximum of the objective <script type="math/tex">N(\theta) = E_{\zeta}(f(\theta,\zeta))</script></p>

<h2 id="conclusion">Conclusion</h2>

<p>CEM performs really well on low-dim problems!
Not covered in sutton although used widely as a benchmark in continuous control problems…</p>

<p>MM monotically increases expected reward! Resources: Andrew ng doc, link to proofs…</p>

<h3 id="conclusion-1">Conclusion</h3>

<p>Works embarassingly well on problems with a small number of parameters…
CEM and CMA are derivative-free algorithms, hence their sample complexity scales unfavorably with the number of parameters, and they performed poorly on the larger problems.</p>

</section>
</article>

<!-- Post navigation -->


<!-- Disqus -->


    </div>
    
<script src="/blog/js/katex_init.js"></script>



<footer class="site-footer">
	<p class="text">© 2016 ・ <a class="plain" href="http://dustintran.com">dustintran.com</a>
</p>
</footer>


  </body>
</html>
